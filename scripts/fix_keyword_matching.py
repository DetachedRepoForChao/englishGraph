#!/usr/bin/env python3
"""
ä¿®å¤å…³é”®è¯åŒ¹é…é—®é¢˜
è¯Šæ–­å¹¶ä¿®å¤AI Agentä¸€ç›´è¿”å›ç›¸åŒç½®ä¿¡åº¦çš„é—®é¢˜
"""
import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import requests
import json
from backend.services.database import neo4j_service
from backend.services.nlp_service import nlp_service

def diagnose_keyword_matching():
    """è¯Šæ–­å…³é”®è¯åŒ¹é…é—®é¢˜"""
    print("ğŸ” è¯Šæ–­å…³é”®è¯åŒ¹é…é—®é¢˜")
    print("="*50)
    
    # 1. æ£€æŸ¥æ•°æ®åº“è¿æ¥
    print("1ï¸âƒ£ æ£€æŸ¥æ•°æ®åº“è¿æ¥...")
    if neo4j_service.connect():
        print("âœ… æ•°æ®åº“è¿æ¥æ­£å¸¸")
        
        # è·å–æ•°æ®åº“ä¸­çš„çŸ¥è¯†ç‚¹
        db_knowledge_points = neo4j_service.search_knowledge_points("")
        print(f"ğŸ“š æ•°æ®åº“ä¸­çš„çŸ¥è¯†ç‚¹ ({len(db_knowledge_points)}ä¸ª):")
        for i, kp in enumerate(db_knowledge_points, 1):
            print(f"   {i:2d}. {kp.get('name', 'æœªçŸ¥')} (ID: {kp.get('id', 'æœªçŸ¥')})")
    else:
        print("âŒ æ•°æ®åº“è¿æ¥å¤±è´¥")
        return False
    
    # 2. æ£€æŸ¥å…³é”®è¯æ¨¡å¼åº“
    print(f"\n2ï¸âƒ£ æ£€æŸ¥å…³é”®è¯æ¨¡å¼åº“...")
    keyword_patterns = nlp_service.keyword_patterns
    print(f"ğŸ“‹ å…³é”®è¯åº“ä¸­çš„çŸ¥è¯†ç‚¹ ({len(keyword_patterns)}ä¸ª):")
    for i, kp_name in enumerate(keyword_patterns.keys(), 1):
        keywords_count = len(keyword_patterns[kp_name])
        print(f"   {i:2d}. {kp_name} ({keywords_count}ä¸ªå…³é”®è¯)")
    
    # 3. æ£€æŸ¥åŒ¹é…é—®é¢˜
    print(f"\n3ï¸âƒ£ æ£€æŸ¥åç§°åŒ¹é…é—®é¢˜...")
    db_names = {kp.get('name', '') for kp in db_knowledge_points}
    pattern_names = set(keyword_patterns.keys())
    
    print(f"ğŸ” æ•°æ®åº“çŸ¥è¯†ç‚¹: {sorted(db_names)}")
    print(f"ğŸ” å…³é”®è¯åº“çŸ¥è¯†ç‚¹: {sorted(pattern_names)}")
    
    # æ‰¾å‡ºä¸åŒ¹é…çš„çŸ¥è¯†ç‚¹
    missing_in_patterns = db_names - pattern_names
    missing_in_db = pattern_names - db_names
    
    if missing_in_patterns:
        print(f"âŒ æ•°æ®åº“ä¸­æœ‰ä½†å…³é”®è¯åº“ä¸­æ²¡æœ‰çš„çŸ¥è¯†ç‚¹:")
        for name in sorted(missing_in_patterns):
            print(f"   â€¢ {name}")
    
    if missing_in_db:
        print(f"âš ï¸ å…³é”®è¯åº“ä¸­æœ‰ä½†æ•°æ®åº“ä¸­æ²¡æœ‰çš„çŸ¥è¯†ç‚¹:")
        for name in sorted(missing_in_db):
            print(f"   â€¢ {name}")
    
    return True

def test_keyword_matching():
    """æµ‹è¯•å…³é”®è¯åŒ¹é…åŠŸèƒ½"""
    print(f"\n4ï¸âƒ£ æµ‹è¯•å…³é”®è¯åŒ¹é…åŠŸèƒ½...")
    
    test_cases = [
        ("She goes to school every day.", "ä¸€èˆ¬ç°åœ¨æ—¶"),
        ("I have already finished my homework.", "ç°åœ¨å®Œæˆæ—¶"),
        ("The book which is on the table belongs to me.", "å®šè¯­ä»å¥"),
        ("Yesterday I went to the park.", "ä¸€èˆ¬è¿‡å»æ—¶")
    ]
    
    for question_text, expected_kp in test_cases:
        print(f"\nğŸ§ª æµ‹è¯•: {question_text[:40]}...")
        print(f"   æœŸæœ›çŸ¥è¯†ç‚¹: {expected_kp}")
        
        # ç›´æ¥æµ‹è¯•å…³é”®è¯åŒ¹é…
        if expected_kp in nlp_service.keyword_patterns:
            score, matched_keywords = nlp_service._keyword_matching_score(question_text, expected_kp)
            print(f"   ğŸ¯ åŒ¹é…åˆ†æ•°: {score:.3f}")
            print(f"   ğŸ”‘ åŒ¹é…å…³é”®è¯: {matched_keywords}")
        else:
            print(f"   âŒ å…³é”®è¯åº“ä¸­æ²¡æœ‰ '{expected_kp}'")

def fix_keyword_patterns():
    """ä¿®å¤å…³é”®è¯æ¨¡å¼åº“"""
    print(f"\n5ï¸âƒ£ ä¿®å¤å…³é”®è¯æ¨¡å¼åº“...")
    
    # è·å–æ•°æ®åº“ä¸­çš„å®é™…çŸ¥è¯†ç‚¹åç§°
    if not neo4j_service.connect():
        print("âŒ æ— æ³•è¿æ¥æ•°æ®åº“")
        return False
    
    db_knowledge_points = neo4j_service.search_knowledge_points("")
    db_kp_names = [kp.get('name', '') for kp in db_knowledge_points]
    
    print(f"ğŸ“š æ•°æ®åº“ä¸­çš„çŸ¥è¯†ç‚¹: {db_kp_names}")
    
    # æ›´æ–°å…³é”®è¯æ¨¡å¼åº“ä»¥åŒ¹é…æ•°æ®åº“ä¸­çš„çŸ¥è¯†ç‚¹åç§°
    updated_patterns = {}
    
    for kp_name in db_kp_names:
        if kp_name in nlp_service.keyword_patterns:
            # å·²å­˜åœ¨ï¼Œç›´æ¥å¤åˆ¶
            updated_patterns[kp_name] = nlp_service.keyword_patterns[kp_name]
            print(f"âœ… ä¿ç•™ç°æœ‰æ¨¡å¼: {kp_name}")
        else:
            # æ–°å¢çŸ¥è¯†ç‚¹ï¼Œåˆ›å»ºåŸºç¡€å…³é”®è¯
            if "æ—¶æ€" in kp_name:
                updated_patterns[kp_name] = ["æ—¶æ€", "åŠ¨è¯", "tense"]
            elif "è¯­æ³•" in kp_name:
                updated_patterns[kp_name] = ["è¯­æ³•", "grammar", "å¥å‹"]
            elif "ä»å¥" in kp_name:
                updated_patterns[kp_name] = ["ä»å¥", "clause", "å¥å­"]
            elif "è¯­æ€" in kp_name:
                updated_patterns[kp_name] = ["è¯­æ€", "voice", "ä¸»åŠ¨", "è¢«åŠ¨"]
            elif "æ¯”è¾ƒ" in kp_name:
                updated_patterns[kp_name] = ["æ¯”è¾ƒ", "than", "more", "most"]
            else:
                updated_patterns[kp_name] = [kp_name.lower()]
            
            print(f"ğŸ†• æ–°å¢æ¨¡å¼: {kp_name} â†’ {updated_patterns[kp_name]}")
    
    # æ›´æ–°NLPæœåŠ¡çš„å…³é”®è¯æ¨¡å¼åº“
    nlp_service.keyword_patterns = updated_patterns
    
    print(f"âœ… å…³é”®è¯æ¨¡å¼åº“å·²æ›´æ–°ï¼Œå…± {len(updated_patterns)} ä¸ªçŸ¥è¯†ç‚¹")
    neo4j_service.close()
    return True

def test_fixed_matching():
    """æµ‹è¯•ä¿®å¤åçš„åŒ¹é…æ•ˆæœ"""
    print(f"\n6ï¸âƒ£ æµ‹è¯•ä¿®å¤åçš„åŒ¹é…æ•ˆæœ...")
    
    test_questions = [
        {
            "content": "She goes to school every day.",
            "question_type": "é€‰æ‹©é¢˜",
            "expected": "ä¸€èˆ¬ç°åœ¨æ—¶"
        },
        {
            "content": "I have already finished my homework.",
            "question_type": "é€‰æ‹©é¢˜", 
            "expected": "ç°åœ¨å®Œæˆæ—¶"
        },
        {
            "content": "The letter was written by Tom.",
            "question_type": "é€‰æ‹©é¢˜",
            "expected": "è¢«åŠ¨è¯­æ€"
        }
    ]
    
    for i, test in enumerate(test_questions, 1):
        print(f"\nğŸ§ª æµ‹è¯• {i}: {test['content'][:40]}...")
        
        # è°ƒç”¨APIæµ‹è¯•
        try:
            response = requests.post(
                "http://localhost:8000/api/ai-agent/auto-annotate",
                json={"question": test},
                timeout=10
            )
            
            if response.status_code == 200:
                result = response.json()
                suggestions = result.get("suggestions", [])
                
                print(f"   ğŸ’¡ AIæ¨è ({len(suggestions)}ä¸ª):")
                for j, suggestion in enumerate(suggestions[:3], 1):
                    kp_name = suggestion['knowledge_point_name']
                    confidence = suggestion['confidence']
                    keywords = suggestion.get('matched_keywords', [])
                    print(f"      {j}. {kp_name} (ç½®ä¿¡åº¦: {confidence:.3f}) å…³é”®è¯: {keywords}")
                
                # æ£€æŸ¥æ˜¯å¦æ­£ç¡®è¯†åˆ«æœŸæœ›çš„çŸ¥è¯†ç‚¹
                expected = test['expected']
                found_expected = any(expected in s['knowledge_point_name'] for s in suggestions)
                
                if found_expected:
                    print(f"   âœ… æˆåŠŸè¯†åˆ«æœŸæœ›çŸ¥è¯†ç‚¹: {expected}")
                else:
                    print(f"   âŒ æœªè¯†åˆ«æœŸæœ›çŸ¥è¯†ç‚¹: {expected}")
            else:
                print(f"   âŒ APIè°ƒç”¨å¤±è´¥: {response.status_code}")
        except Exception as e:
            print(f"   âŒ æµ‹è¯•å¤±è´¥: {e}")

def main():
    """ä¸»ä¿®å¤å‡½æ•°"""
    print("ğŸ”§ AI Agentå…³é”®è¯åŒ¹é…é—®é¢˜ä¿®å¤")
    print("="*50)
    
    # è¯Šæ–­é—®é¢˜
    if not diagnose_keyword_matching():
        return
    
    # æµ‹è¯•å½“å‰åŒ¹é…æ•ˆæœ
    test_keyword_matching()
    
    # ä¿®å¤å…³é”®è¯æ¨¡å¼åº“
    if fix_keyword_patterns():
        print("\nğŸ‰ å…³é”®è¯æ¨¡å¼åº“ä¿®å¤å®Œæˆ")
        
        # æµ‹è¯•ä¿®å¤æ•ˆæœ
        test_fixed_matching()
        
        print("\nğŸ“Š ä¿®å¤æ€»ç»“:")
        print("âœ… å…³é”®è¯æ¨¡å¼åº“å·²æ›´æ–°")
        print("âœ… æ•°æ®åº“çŸ¥è¯†ç‚¹åç§°å·²åŒ¹é…")
        print("ğŸ’¡ ç°åœ¨AI Agentåº”è¯¥èƒ½æ­£ç¡®è¯†åˆ«ä¸åŒçŸ¥è¯†ç‚¹äº†")
        print("ğŸš€ è¯·é‡æ–°æµ‹è¯•é¢˜ç›®æ ‡æ³¨åŠŸèƒ½")
    else:
        print("âŒ ä¿®å¤å¤±è´¥")

if __name__ == "__main__":
    main()
