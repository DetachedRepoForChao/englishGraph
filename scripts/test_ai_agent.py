#!/usr/bin/env python3
"""
AI AgentåŠŸèƒ½æµ‹è¯•è„šæœ¬
æµ‹è¯•è‡ªåŠ¨æ ‡æ³¨åŠŸèƒ½æ˜¯å¦æ­£å¸¸å·¥ä½œ
"""
import sys
import os
import asyncio
import json
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import logging
from backend.services.database import neo4j_service
from backend.services.ai_agent_service import ai_agent_service
from backend.models.schema import Question

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


async def test_single_auto_annotation():
    """æµ‹è¯•å•ä¸ªé¢˜ç›®è‡ªåŠ¨æ ‡æ³¨"""
    logger.info("=== æµ‹è¯•å•ä¸ªé¢˜ç›®è‡ªåŠ¨æ ‡æ³¨ ===")
    
    # åˆ›å»ºæµ‹è¯•é¢˜ç›®
    test_question = Question(
        content="She _____ to school every day. A) go B) goes C) going D) gone",
        question_type="é€‰æ‹©é¢˜",
        options=["go", "goes", "going", "gone"],
        answer="B",
        analysis="ä¸»è¯­Sheæ˜¯ç¬¬ä¸‰äººç§°å•æ•°ï¼ŒåŠ¨è¯éœ€è¦ç”¨ç¬¬ä¸‰äººç§°å•æ•°å½¢å¼goes",
        source="AI Agentæµ‹è¯•",
        difficulty="easy"
    )
    
    try:
        # æ‰§è¡Œè‡ªåŠ¨æ ‡æ³¨
        result = await ai_agent_service.auto_annotate_question(test_question)
        
        logger.info(f"æ ‡æ³¨ç»“æžœ: {json.dumps(result, ensure_ascii=False, indent=2)}")
        
        if result.get("status") == "completed":
            suggestions = result.get("auto_annotations", [])
            applied = result.get("applied_annotations", [])
            
            logger.info(f"âœ… è‡ªåŠ¨æ ‡æ³¨æˆåŠŸ")
            logger.info(f"   - çŸ¥è¯†ç‚¹å»ºè®®æ•°: {len(suggestions)}")
            logger.info(f"   - è‡ªåŠ¨åº”ç”¨æ•°: {len(applied)}")
            
            for i, suggestion in enumerate(suggestions[:3], 1):
                logger.info(f"   {i}. {suggestion['knowledge_point_name']} "
                          f"(ç½®ä¿¡åº¦: {suggestion['confidence']:.3f}, "
                          f"å†³ç­–åˆ†æ•°: {suggestion['decision_score']:.3f})")
            
            return True
        else:
            logger.error("âŒ è‡ªåŠ¨æ ‡æ³¨å¤±è´¥")
            return False
            
    except Exception as e:
        logger.error(f"âŒ æµ‹è¯•è¿‡ç¨‹å‡ºé”™: {e}")
        return False


async def test_batch_auto_annotation():
    """æµ‹è¯•æ‰¹é‡è‡ªåŠ¨æ ‡æ³¨"""
    logger.info("\n=== æµ‹è¯•æ‰¹é‡è‡ªåŠ¨æ ‡æ³¨ ===")
    
    # åŠ è½½ç¤ºä¾‹é¢˜ç›®
    try:
        with open("data/sample_questions/import_example.json", "r", encoding="utf-8") as f:
            questions_data = json.load(f)
        
        test_questions = []
        for q_data in questions_data[:3]:  # åªæµ‹è¯•å‰3é“é¢˜
            question = Question(**q_data)
            test_questions.append(question)
        
        logger.info(f"å‡†å¤‡æ‰¹é‡æ ‡æ³¨ {len(test_questions)} é“é¢˜ç›®")
        
        # æ‰§è¡Œæ‰¹é‡æ ‡æ³¨
        result = await ai_agent_service.batch_auto_annotate(test_questions)
        
        logger.info(f"æ‰¹é‡æ ‡æ³¨ç»“æžœ:")
        logger.info(f"   - æ€»é¢˜ç›®æ•°: {result['total_questions']}")
        logger.info(f"   - æˆåŠŸæ•°: {result['success_count']}")
        logger.info(f"   - å¤±è´¥æ•°: {result['error_count']}")
        logger.info(f"   - æˆåŠŸçŽ‡: {result['success_rate']:.2%}")
        
        if result['success_rate'] > 0.5:
            logger.info("âœ… æ‰¹é‡æ ‡æ³¨æµ‹è¯•é€šè¿‡")
            return True
        else:
            logger.error("âŒ æ‰¹é‡æ ‡æ³¨æˆåŠŸçŽ‡è¿‡ä½Ž")
            return False
            
    except Exception as e:
        logger.error(f"âŒ æ‰¹é‡æ ‡æ³¨æµ‹è¯•å¤±è´¥: {e}")
        return False


async def test_ai_agent_configuration():
    """æµ‹è¯•AI Agenté…ç½®åŠŸèƒ½"""
    logger.info("\n=== æµ‹è¯•AI Agenté…ç½® ===")
    
    try:
        # èŽ·å–å½“å‰é…ç½®
        current_config = ai_agent_service.get_configuration()
        logger.info(f"å½“å‰é…ç½®: {current_config}")
        
        # æ›´æ–°é…ç½®
        new_config = {
            "confidence_threshold": 0.5,
            "max_auto_annotations": 3,
            "learning_enabled": True
        }
        
        ai_agent_service.update_configuration(new_config)
        
        # éªŒè¯é…ç½®æ›´æ–°
        updated_config = ai_agent_service.get_configuration()
        logger.info(f"æ›´æ–°åŽé…ç½®: {updated_config}")
        
        if updated_config["confidence_threshold"] == 0.5:
            logger.info("âœ… é…ç½®æ›´æ–°æµ‹è¯•é€šè¿‡")
            
            # æ¢å¤åŽŸå§‹é…ç½®
            ai_agent_service.update_configuration(current_config)
            return True
        else:
            logger.error("âŒ é…ç½®æ›´æ–°å¤±è´¥")
            return False
            
    except Exception as e:
        logger.error(f"âŒ é…ç½®æµ‹è¯•å¤±è´¥: {e}")
        return False


async def test_decision_scoring():
    """æµ‹è¯•AI Agentå†³ç­–è¯„åˆ†æœºåˆ¶"""
    logger.info("\n=== æµ‹è¯•å†³ç­–è¯„åˆ†æœºåˆ¶ ===")
    
    try:
        # åˆ›å»ºä¸åŒç±»åž‹çš„æµ‹è¯•é¢˜ç›®
        test_cases = [
            {
                "question": Question(
                    content="She is playing basketball now.",
                    question_type="é€‰æ‹©é¢˜",
                    answer="çŽ°åœ¨è¿›è¡Œæ—¶",
                    difficulty="medium"
                ),
                "expected_kp": "çŽ°åœ¨è¿›è¡Œæ—¶"
            },
            {
                "question": Question(
                    content="I went to school yesterday.",
                    question_type="é€‰æ‹©é¢˜", 
                    answer="ä¸€èˆ¬è¿‡åŽ»æ—¶",
                    difficulty="easy"
                ),
                "expected_kp": "ä¸€èˆ¬è¿‡åŽ»æ—¶"
            }
        ]
        
        correct_predictions = 0
        total_tests = len(test_cases)
        
        for i, test_case in enumerate(test_cases, 1):
            logger.info(f"æµ‹è¯•æ¡ˆä¾‹ {i}: {test_case['question'].content}")
            
            result = await ai_agent_service.auto_annotate_question(test_case["question"])
            
            if result.get("status") == "completed":
                suggestions = result.get("auto_annotations", [])
                
                # æ£€æŸ¥æ˜¯å¦åŒ…å«é¢„æœŸçš„çŸ¥è¯†ç‚¹
                found_expected = any(
                    test_case["expected_kp"] in suggestion.get("knowledge_point_name", "")
                    for suggestion in suggestions
                )
                
                if found_expected:
                    correct_predictions += 1
                    logger.info(f"   âœ… æ­£ç¡®è¯†åˆ«äº†é¢„æœŸçŸ¥è¯†ç‚¹")
                else:
                    logger.info(f"   âŒ æœªè¯†åˆ«é¢„æœŸçŸ¥è¯†ç‚¹: {test_case['expected_kp']}")
                    logger.info(f"   å®žé™…å»ºè®®: {[s['knowledge_point_name'] for s in suggestions[:3]]}")
        
        accuracy = correct_predictions / total_tests
        logger.info(f"\nå†³ç­–å‡†ç¡®çŽ‡: {accuracy:.2%} ({correct_predictions}/{total_tests})")
        
        if accuracy >= 0.5:
            logger.info("âœ… å†³ç­–è¯„åˆ†æµ‹è¯•é€šè¿‡")
            return True
        else:
            logger.error("âŒ å†³ç­–å‡†ç¡®çŽ‡è¿‡ä½Ž")
            return False
            
    except Exception as e:
        logger.error(f"âŒ å†³ç­–è¯„åˆ†æµ‹è¯•å¤±è´¥: {e}")
        return False


async def run_comprehensive_ai_agent_test():
    """è¿è¡ŒAI Agentç»¼åˆæµ‹è¯•"""
    logger.info("ðŸš€ å¼€å§‹AI AgentåŠŸèƒ½æµ‹è¯•")
    
    # ç¡®ä¿æ•°æ®åº“è¿žæŽ¥
    if not neo4j_service.connect():
        logger.error("âŒ æ— æ³•è¿žæŽ¥åˆ°æ•°æ®åº“ï¼Œæµ‹è¯•ç»ˆæ­¢")
        return False
    
    try:
        test_results = {}
        
        # æ‰§è¡Œå„é¡¹æµ‹è¯•
        test_results['single_annotation'] = await test_single_auto_annotation()
        test_results['batch_annotation'] = await test_batch_auto_annotation()
        test_results['configuration'] = await test_ai_agent_configuration()
        test_results['decision_scoring'] = await test_decision_scoring()
        
        # æ±‡æ€»ç»“æžœ
        logger.info("\n" + "="*60)
        logger.info("ðŸŽ¯ AI Agentæµ‹è¯•ç»“æžœæ±‡æ€»")
        logger.info("="*60)
        
        passed = 0
        total = 0
        
        for test_name, result in test_results.items():
            total += 1
            if result:
                passed += 1
                logger.info(f"âœ… {test_name.replace('_', ' ').title()}: é€šè¿‡")
            else:
                logger.info(f"âŒ {test_name.replace('_', ' ').title()}: å¤±è´¥")
        
        success_rate = passed / total
        logger.info(f"\nðŸŽ‰ AI Agentæµ‹è¯•é€šè¿‡çŽ‡: {success_rate:.1%} ({passed}/{total})")
        
        if success_rate >= 0.75:
            logger.info("ðŸŽŠ AI AgentåŠŸèƒ½æµ‹è¯•å…¨é¢é€šè¿‡ï¼")
        elif success_rate >= 0.5:
            logger.info("âš ï¸  AI AgentåŸºæœ¬åŠŸèƒ½æ­£å¸¸ï¼Œéƒ¨åˆ†åŠŸèƒ½éœ€è¦ä¼˜åŒ–")
        else:
            logger.info("âŒ AI AgentåŠŸèƒ½å­˜åœ¨è¾ƒå¤šé—®é¢˜ï¼Œéœ€è¦æ£€æŸ¥")
        
        return success_rate >= 0.5
        
    finally:
        neo4j_service.close()


if __name__ == "__main__":
    asyncio.run(run_comprehensive_ai_agent_test())
