# AI Agentæ‰“æ ‡ç­¾å’Œæƒé‡è®¡ç®—åŸç†è¯¦è§£

## ğŸ“‹ ç›®å½•
- [æ•´ä½“æ¶æ„](#æ•´ä½“æ¶æ„)
- [æ ‡æ³¨å†³ç­–æµç¨‹](#æ ‡æ³¨å†³ç­–æµç¨‹)
- [æƒé‡è®¡ç®—ç®—æ³•](#æƒé‡è®¡ç®—ç®—æ³•)
- [å…³é”®è¯åŒ¹é…æœºåˆ¶](#å…³é”®è¯åŒ¹é…æœºåˆ¶)
- [å¤šå› ç´ å†³ç­–æ¨¡å‹](#å¤šå› ç´ å†³ç­–æ¨¡å‹)
- [å®é™…åº”ç”¨ç¤ºä¾‹](#å®é™…åº”ç”¨ç¤ºä¾‹)
- [å‚æ•°è°ƒä¼˜æŒ‡å—](#å‚æ•°è°ƒä¼˜æŒ‡å—)

## æ•´ä½“æ¶æ„

### ğŸ—ï¸ AI Agentå·¥ä½œæµç¨‹

```mermaid
graph TD
    A[è¾“å…¥é¢˜ç›®] --> B[NLPé¢„å¤„ç†]
    B --> C[å…³é”®è¯æå–]
    C --> D[çŸ¥è¯†ç‚¹å€™é€‰ç”Ÿæˆ]
    D --> E[å¤šå› ç´ å†³ç­–è¯„åˆ†]
    E --> F{ç½®ä¿¡åº¦æ£€æŸ¥}
    F -->|â‰¥é˜ˆå€¼| G[è‡ªåŠ¨åº”ç”¨æ ‡æ³¨]
    F -->|<é˜ˆå€¼| H[æ¨èäººå·¥å®¡æ ¸]
    G --> I[ä¿å­˜åˆ°å›¾æ•°æ®åº“]
    H --> J[ç­‰å¾…äººå·¥ç¡®è®¤]
    J --> I
```

### ğŸ§  æ ¸å¿ƒç»„ä»¶

1. **NLPæœåŠ¡** (`nlp_service.py`) - è´Ÿè´£æ–‡æœ¬åˆ†æå’Œåˆå§‹æ¨è
2. **AI AgentæœåŠ¡** (`ai_agent_service.py`) - è´Ÿè´£æ™ºèƒ½å†³ç­–å’Œæƒé‡è®¡ç®—
3. **å†³ç­–å¼•æ“** - å¤šå› ç´ ç»¼åˆè¯„ä¼°ç®—æ³•
4. **æƒé‡è°ƒæ•´å™¨** - åŠ¨æ€æƒé‡è®¡ç®—å’Œä¼˜åŒ–

## æ ‡æ³¨å†³ç­–æµç¨‹

### ğŸ” ç¬¬ä¸€é˜¶æ®µï¼šNLPåˆ†æ

#### 1. æ–‡æœ¬é¢„å¤„ç†
```python
def _preprocess_text(self, text: str) -> str:
    # ç§»é™¤ç‰¹æ®Šå­—ç¬¦ï¼Œä¿ç•™ä¸­è‹±æ–‡å’Œæ•°å­—
    text = re.sub(r'[^\w\s\u4e00-\u9fff]', ' ', text)
    # è½¬æ¢ä¸ºå°å†™
    text = text.lower()
    # ç§»é™¤å¤šä½™ç©ºæ ¼
    text = re.sub(r'\s+', ' ', text).strip()
    return text
```

#### 2. å…³é”®è¯æå–
```python
def _extract_keywords(self, text: str) -> List[str]:
    # ä¸­æ–‡åˆ†è¯ (ä½¿ç”¨jieba)
    chinese_words = jieba.lcut(processed_text)
    
    # è‹±æ–‡å•è¯æå–
    english_words = re.findall(r'[a-zA-Z]+', processed_text)
    
    # åˆå¹¶å…³é”®è¯å¹¶è¿‡æ»¤
    keywords = [word for word in (chinese_words + english_words) if len(word) > 1]
    return keywords
```

#### 3. å…³é”®è¯åŒ¹é…è¯„åˆ†
```python
def _keyword_matching_score(self, question_text: str, knowledge_point: str):
    patterns = self.keyword_patterns[knowledge_point]
    matched_keywords = []
    score = 0.0
    
    for pattern in patterns:
        if pattern.lower() in question_text.lower():
            matched_keywords.append(pattern)
            # é•¿å…³é”®è¯æƒé‡æ›´é«˜
            score += 2.0 if len(pattern) > 5 else 1.0
    
    # å½’ä¸€åŒ–åˆ†æ•° (0-1ä¹‹é—´)
    max_possible_score = len(patterns) * 2.0
    normalized_score = min(score / max_possible_score, 1.0)
    
    return normalized_score, matched_keywords
```

### ğŸ¯ ç¬¬äºŒé˜¶æ®µï¼šAI Agentå†³ç­–

#### å¤šå› ç´ å†³ç­–æ¨¡å‹
```python
async def _calculate_decision_score(self, question, suggestion, base_confidence):
    score = base_confidence  # åŸºç¡€ç½®ä¿¡åº¦
    
    # 1. é¢˜ç›®ç±»å‹åŒ¹é…åº¦åŠ æƒ (+0.0~0.3)
    score += self._get_question_type_boost(question.question_type, suggestion.kp_name)
    
    # 2. å…³é”®è¯åŒ¹é…å¼ºåº¦åŠ æƒ (+0.0~0.2)
    score += self._get_keyword_match_boost(question.content, suggestion.keywords)
    
    # 3. å†å²å‡†ç¡®ç‡åŠ æƒ (+0.0~0.1)
    if self.learning_enabled:
        score += await self._get_historical_accuracy_boost(suggestion.kp_id, question.question_type)
    
    # 4. é¢˜ç›®éš¾åº¦åŒ¹é…åº¦ (+0.0~0.1)
    score += self._get_difficulty_match_boost(question.difficulty, suggestion.kp_name)
    
    # 5. è¿‡åº¦æ ‡æ³¨æƒ©ç½š (-0.0~0.3)
    score -= await self._get_over_annotation_penalty(question)
    
    return max(0.0, min(1.0, score))  # é™åˆ¶åœ¨0-1ä¹‹é—´
```

## æƒé‡è®¡ç®—ç®—æ³•

### ğŸ† æƒé‡è®¡ç®—çš„äº”ä¸ªç»´åº¦

#### 1. é¢˜ç›®ç±»å‹åŒ¹é…åº¦ (Type Boost)
```python
def _get_question_type_boost(self, question_type: str, kp_name: str) -> float:
    type_mappings = {
        "é€‰æ‹©é¢˜": {
            "è¯­æ³•": 0.2,    # é€‰æ‹©é¢˜å¾ˆé€‚åˆè€ƒæŸ¥è¯­æ³•
            "æ—¶æ€": 0.2,    # æ—¶æ€æ˜¯é€‰æ‹©é¢˜å¸¸è€ƒç‚¹
            "è¯æ±‡": 0.1,    # è¯æ±‡é€‰æ‹©é¢˜è¾ƒå¸¸è§
            "è¯­æ€": 0.2     # è¯­æ€é€‰æ‹©é¢˜å¾ˆå¸¸è§
        },
        "å¡«ç©ºé¢˜": {
            "æ—¶æ€": 0.3,    # å¡«ç©ºé¢˜æœ€é€‚åˆè€ƒæŸ¥æ—¶æ€
            "ä»‹è¯": 0.3,    # ä»‹è¯å¡«ç©ºå¾ˆå¸¸è§
            "è¯å½¢å˜åŒ–": 0.2, # è¯å½¢å˜åŒ–é€‚åˆå¡«ç©º
            "è¯­æ³•": 0.1
        },
        "é˜…è¯»ç†è§£": {
            "é˜…è¯»æŠ€å·§": 0.3,
            "è¯æ±‡ç†è§£": 0.2,
            "è¯­æ³•ç†è§£": 0.1
        },
        "ç¿»è¯‘é¢˜": {
            "è¯­æ³•": 0.3,    # ç¿»è¯‘æœ€è€ƒæŸ¥è¯­æ³•
            "è¯æ±‡": 0.2,
            "å¥å‹": 0.2
        }
    }
    
    # å¦‚æœé¢˜ç›®ç±»å‹å’ŒçŸ¥è¯†ç‚¹åŒ¹é…ï¼Œè¿”å›å¯¹åº”æƒé‡
    if question_type in type_mappings:
        for keyword, boost in type_mappings[question_type].items():
            if keyword in kp_name:
                return boost
    return 0.0
```

**ç¤ºä¾‹**:
- é¢˜ç›®: "She _____ to school every day." (é€‰æ‹©é¢˜)
- çŸ¥è¯†ç‚¹: "åŠ¨è¯æ—¶æ€"
- åŒ¹é…: "é€‰æ‹©é¢˜" + "æ—¶æ€" â†’ åŠ æƒ +0.2

#### 2. å…³é”®è¯åŒ¹é…å¼ºåº¦ (Keyword Boost)
```python
def _get_keyword_match_boost(self, question_content: str, matched_keywords: List[str]) -> float:
    if not matched_keywords:
        return 0.0
    
    # è®¡ç®—åŒ¹é…å¯†åº¦
    total_matches = 0
    for keyword in matched_keywords:
        count = question_content.lower().count(keyword.lower())
        total_matches += count
    
    # æ ¹æ®åŒ¹é…å¯†åº¦ç»™å‡ºåŠ æƒ
    content_length = len(question_content.split())
    match_density = total_matches / max(content_length, 1)
    
    return min(match_density * 0.3, 0.2)  # æœ€å¤šåŠ 0.2åˆ†
```

**ç¤ºä¾‹**:
- é¢˜ç›®: "Yesterday I went to the park"
- åŒ¹é…å…³é”®è¯: ["yesterday"]
- åŒ¹é…å¯†åº¦: 1/7 = 0.14
- åŠ æƒ: 0.14 * 0.3 = 0.042

#### 3. å†å²å‡†ç¡®ç‡åŠ æƒ (History Boost)
```python
async def _get_historical_accuracy_boost(self, kp_id: str, question_type: str) -> float:
    # æŸ¥è¯¢è¯¥çŸ¥è¯†ç‚¹åœ¨ç›¸åŒé¢˜ç›®ç±»å‹ä¸Šçš„å†å²å‡†ç¡®ç‡
    # å¦‚æœå†å²å‡†ç¡®ç‡é«˜ï¼Œç»™äºˆæ­£å‘åŠ æƒ
    # å½“å‰ç‰ˆæœ¬è¿”å›å›ºå®šå€¼ï¼Œå®é™…åº”ç”¨ä¸­åŸºäºç”¨æˆ·åé¦ˆæ•°æ®è®¡ç®—
    return 0.1
```

#### 4. é¢˜ç›®éš¾åº¦åŒ¹é…åº¦ (Difficulty Boost)
```python
def _get_difficulty_match_boost(self, question_difficulty: str, kp_name: str) -> float:
    difficulty_mappings = {
        "easy": ["åŸºç¡€", "ç®€å•", "å…¥é—¨"],
        "medium": ["ä¸­çº§", "ä¸€èˆ¬", "æ ‡å‡†"],
        "hard": ["é«˜çº§", "å¤æ‚", "å›°éš¾"]
    }
    
    if question_difficulty in difficulty_mappings:
        keywords = difficulty_mappings[question_difficulty]
        for keyword in keywords:
            if keyword in kp_name:
                return 0.1
    return 0.0
```

#### 5. è¿‡åº¦æ ‡æ³¨æƒ©ç½š (Over-annotation Penalty)
```python
async def _get_over_annotation_penalty(self, question: Question) -> float:
    # å¦‚æœé¢˜ç›®å·²ç»æœ‰å¾ˆå¤šæ ‡æ³¨ï¼Œé™ä½æ–°æ ‡æ³¨çš„åˆ†æ•°
    existing_annotations = neo4j_service.find_knowledge_points_by_question(question.id)
    annotation_count = len(existing_annotations)
    
    if annotation_count >= 3:
        return 0.1 * (annotation_count - 2)  # è¶…è¿‡3ä¸ªæ ‡æ³¨å¼€å§‹æƒ©ç½š
    return 0.0
```

### ğŸ“Š æƒé‡è®¡ç®—å…¬å¼

**æœ€ç»ˆå†³ç­–åˆ†æ•°** = åŸºç¡€ç½®ä¿¡åº¦ + é¢˜ç›®ç±»å‹åŠ æƒ + å…³é”®è¯åŒ¹é…åŠ æƒ + å†å²å‡†ç¡®ç‡åŠ æƒ + éš¾åº¦åŒ¹é…åŠ æƒ - è¿‡åº¦æ ‡æ³¨æƒ©ç½š

```
Decision_Score = Base_Confidence 
               + Type_Boost (0~0.3)
               + Keyword_Boost (0~0.2) 
               + History_Boost (0~0.1)
               + Difficulty_Boost (0~0.1)
               - Over_Annotation_Penalty (0~0.3)
```

**æƒé‡è°ƒæ•´è§„åˆ™**:
```python
# æœ€ç»ˆæƒé‡ = min(å†³ç­–åˆ†æ•°, 1.0)
adjusted_weight = min(decision_score, 1.0)

# è‡ªåŠ¨åº”ç”¨æ¡ä»¶
auto_applied = decision_score >= 0.7  # é«˜ç½®ä¿¡åº¦è‡ªåŠ¨åº”ç”¨
```

## å…³é”®è¯åŒ¹é…æœºåˆ¶

### ğŸ“š å…³é”®è¯åº“ç»“æ„

```python
keyword_patterns = {
    "ä¸€èˆ¬ç°åœ¨æ—¶": [
        # æ—¶é—´æ ‡å¿—è¯
        "always", "usually", "often", "sometimes", "never",
        "every day", "every week", "every month", "every year",
        
        # ä¸­æ–‡æ ‡å¿—è¯
        "æ€»æ˜¯", "é€šå¸¸", "ç»å¸¸", "æœ‰æ—¶", "ä»ä¸", "æ¯å¤©", "æ¯å‘¨",
        
        # è¯­æ³•ç‰¹å¾
        "ç¬¬ä¸‰äººç§°å•æ•°", "åŠ¨è¯åŸå½¢", "does", "do", "goes", "plays"
    ],
    
    "ç°åœ¨å®Œæˆæ—¶": [
        # æ ‡å¿—è¯
        "already", "yet", "just", "ever", "never", "since", "for",
        
        # ä¸­æ–‡æ ‡å¿—è¯  
        "å·²ç»", "è¿˜", "åˆšåˆš", "æ›¾ç»", "ä»æœª", "è‡ªä»", "æŒç»­",
        
        # è¯­æ³•ç‰¹å¾
        "have", "has", "è¿‡å»åˆ†è¯", "finished", "done", "lived"
    ],
    
    "è¢«åŠ¨è¯­æ€": [
        # è¯­æ³•æ ‡å¿—
        "beåŠ¨è¯", "è¿‡å»åˆ†è¯", "by", "è¢«åŠ¨", "passive voice",
        
        # å…·ä½“å½¢å¼
        "was", "were", "is", "are", "am", "been",
        "cleaned", "written", "made", "done", "finished"
    ]
    
    # ... æ›´å¤šçŸ¥è¯†ç‚¹
}
```

### ğŸ” åŒ¹é…ç®—æ³•è¯¦è§£

#### æ­¥éª¤1: æ–‡æœ¬é¢„å¤„ç†
```python
# åŸå§‹é¢˜ç›®
"She _____ to school every day. A) go B) goes C) going D) gone"

# é¢„å¤„ç†å
"she to school every day a go b goes c going d gone"
```

#### æ­¥éª¤2: å…³é”®è¯æ‰«æ
```python
# å¯¹æ¯ä¸ªçŸ¥è¯†ç‚¹çš„å…³é”®è¯è¿›è¡ŒåŒ¹é…
for knowledge_point in all_knowledge_points:
    patterns = keyword_patterns[knowledge_point]
    matched_keywords = []
    
    for pattern in patterns:
        if pattern.lower() in question_text.lower():
            matched_keywords.append(pattern)
```

#### æ­¥éª¤3: è¯„åˆ†è®¡ç®—
```python
# ç¤ºä¾‹ï¼šä¸€èˆ¬ç°åœ¨æ—¶åŒ¹é…
patterns = ["always", "usually", "every day", "goes", ...]
question = "she goes to school every day"

matched = ["every day", "goes"]  # åŒ¹é…åˆ°2ä¸ªå…³é”®è¯
score = 1.0 + 1.0 = 2.0          # çŸ­è¯1åˆ†ï¼Œé•¿è¯2åˆ†
max_score = len(patterns) * 2.0   # æœ€å¤§å¯èƒ½åˆ†æ•°
normalized = 2.0 / max_score      # å½’ä¸€åŒ–åˆ°0-1
```

## å¤šå› ç´ å†³ç­–æ¨¡å‹

### ğŸ¯ å†³ç­–å› å­è¯¦è§£

#### å› å­1: åŸºç¡€ç½®ä¿¡åº¦ (Base Confidence)
- **æ¥æº**: NLPæœåŠ¡çš„å…³é”®è¯åŒ¹é…åˆ†æ•°
- **èŒƒå›´**: 0.0 - 1.0
- **è®¡ç®—**: åŸºäºå…³é”®è¯åŒ¹é…å¯†åº¦å’Œè¯­ä¹‰ç›¸ä¼¼åº¦

```python
# ç»¼åˆåˆ†æ•°è®¡ç®—
total_score = (keyword_score * 0.5 + semantic_score * 0.3 + type_score * 0.2)
```

#### å› å­2: é¢˜ç›®ç±»å‹åŠ æƒ (Type Boost)
- **ç›®çš„**: ä¸åŒé¢˜ç›®ç±»å‹é€‚åˆè€ƒæŸ¥ä¸åŒçŸ¥è¯†ç‚¹
- **æƒé‡**: 0.0 - 0.3

**åŒ¹é…é€»è¾‘**:
```python
# é€‰æ‹©é¢˜ + è¯­æ³•ç±»çŸ¥è¯†ç‚¹ â†’ é«˜æƒé‡
# å¡«ç©ºé¢˜ + æ—¶æ€ç±»çŸ¥è¯†ç‚¹ â†’ é«˜æƒé‡  
# é˜…è¯»ç†è§£ + ç†è§£ç±»çŸ¥è¯†ç‚¹ â†’ é«˜æƒé‡
```

#### å› å­3: å…³é”®è¯åŒ¹é…å¼ºåº¦ (Keyword Boost)
- **ç›®çš„**: å¥–åŠ±å…³é”®è¯åŒ¹é…å¯†åº¦é«˜çš„æ ‡æ³¨
- **æƒé‡**: 0.0 - 0.2

**è®¡ç®—å…¬å¼**:
```python
match_density = total_keyword_matches / question_word_count
keyword_boost = min(match_density * 0.3, 0.2)
```

#### å› å­4: å†å²å‡†ç¡®ç‡åŠ æƒ (History Boost)
- **ç›®çš„**: åŸºäºè¿‡å¾€æ ‡æ³¨æ•ˆæœè°ƒæ•´æƒé‡
- **æƒé‡**: 0.0 - 0.1
- **çŠ¶æ€**: å½“å‰ä¸ºå›ºå®šå€¼ï¼Œæœªæ¥å¯åŸºäºç”¨æˆ·åé¦ˆä¼˜åŒ–

#### å› å­5: éš¾åº¦åŒ¹é…åº¦ (Difficulty Boost)
- **ç›®çš„**: é¢˜ç›®éš¾åº¦ä¸çŸ¥è¯†ç‚¹å¤æ‚åº¦åº”è¯¥åŒ¹é…
- **æƒé‡**: 0.0 - 0.1

```python
# ç®€å•é¢˜ç›® + åŸºç¡€çŸ¥è¯†ç‚¹ â†’ æ­£å‘åŠ æƒ
# å›°éš¾é¢˜ç›® + é«˜çº§çŸ¥è¯†ç‚¹ â†’ æ­£å‘åŠ æƒ
```

#### å› å­6: è¿‡åº¦æ ‡æ³¨æƒ©ç½š (Over-annotation Penalty)
- **ç›®çš„**: é¿å…ç»™å•ä¸ªé¢˜ç›®æ ‡æ³¨è¿‡å¤šçŸ¥è¯†ç‚¹
- **æƒé‡**: 0.0 - 0.3 (è´Ÿå€¼)

```python
if annotation_count >= 3:
    penalty = 0.1 * (annotation_count - 2)  # è¶…è¿‡3ä¸ªå¼€å§‹æƒ©ç½š
```

## å®é™…åº”ç”¨ç¤ºä¾‹

### ğŸ“ ç¤ºä¾‹1: ä¸€èˆ¬ç°åœ¨æ—¶é¢˜ç›®

**é¢˜ç›®**: "She _____ to school every day. A) go B) goes C) going D) gone"

#### ğŸ” NLPåˆ†æé˜¶æ®µ
```python
# 1. å…³é”®è¯æå–
keywords = ["she", "to", "school", "every", "day", "go", "goes", "going", "gone"]

# 2. çŸ¥è¯†ç‚¹åŒ¹é…
knowledge_point = "ä¸€èˆ¬ç°åœ¨æ—¶"
patterns = ["always", "usually", "every day", "goes", "ç¬¬ä¸‰äººç§°å•æ•°"]

# 3. åŒ¹é…ç»“æœ
matched_keywords = ["every day", "goes"]
keyword_score = 2.0 / (len(patterns) * 2.0) = 0.2
```

#### ğŸ¯ AI Agentå†³ç­–é˜¶æ®µ
```python
base_confidence = 0.2  # æ¥è‡ªNLPæœåŠ¡

# å„å› å­è®¡ç®—
type_boost = 0.2       # é€‰æ‹©é¢˜ + æ—¶æ€ â†’ é«˜åŒ¹é…åº¦
keyword_boost = 0.05   # å…³é”®è¯åŒ¹é…å¯†åº¦: 2/9 * 0.3 = 0.067 â†’ min(0.067, 0.2) = 0.067
history_boost = 0.1    # å†å²å‡†ç¡®ç‡åŠ æƒ
difficulty_boost = 0.1 # ç®€å•é¢˜ç›® + åŸºç¡€çŸ¥è¯†ç‚¹
penalty = 0.0          # æ— ç°æœ‰æ ‡æ³¨ï¼Œæ— æƒ©ç½š

# æœ€ç»ˆå†³ç­–åˆ†æ•°
decision_score = 0.2 + 0.2 + 0.05 + 0.1 + 0.1 - 0.0 = 0.65

# æƒé‡è°ƒæ•´
final_weight = min(0.65, 1.0) = 0.65
auto_applied = 0.65 >= 0.7 ? False  # ä¸è‡ªåŠ¨åº”ç”¨ï¼Œéœ€äººå·¥å®¡æ ¸
```

### ğŸ“ ç¤ºä¾‹2: è¢«åŠ¨è¯­æ€é¢˜ç›®

**é¢˜ç›®**: "The letter was written by Tom yesterday."

#### ğŸ” NLPåˆ†æé˜¶æ®µ
```python
# å…³é”®è¯åŒ¹é…
knowledge_point = "è¢«åŠ¨è¯­æ€"
patterns = ["beåŠ¨è¯", "è¿‡å»åˆ†è¯", "by", "was", "were", "written"]
matched_keywords = ["was", "written", "by"]
keyword_score = 3.0 / (len(patterns) * 2.0) = 0.25
```

#### ğŸ¯ AI Agentå†³ç­–é˜¶æ®µ
```python
base_confidence = 0.25

# å„å› å­è®¡ç®—
type_boost = 0.2       # é€‰æ‹©é¢˜ + è¯­æ€
keyword_boost = 0.1    # å…³é”®è¯åŒ¹é…å¯†åº¦è¾ƒé«˜
history_boost = 0.1    
difficulty_boost = 0.0 # å›°éš¾é¢˜ç›®ï¼Œæ— åŸºç¡€åŒ¹é…åŠ æƒ
penalty = 0.0

# æœ€ç»ˆå†³ç­–åˆ†æ•°
decision_score = 0.25 + 0.2 + 0.1 + 0.1 + 0.0 - 0.0 = 0.65

final_weight = 0.65
auto_applied = False   # éœ€äººå·¥å®¡æ ¸
```

### ğŸ“ ç¤ºä¾‹3: é«˜ç½®ä¿¡åº¦è‡ªåŠ¨åº”ç”¨

**é¢˜ç›®**: "Look! The children are playing in the playground."

#### ğŸ” åˆ†æè¿‡ç¨‹
```python
knowledge_point = "ç°åœ¨è¿›è¡Œæ—¶"
matched_keywords = ["look", "are", "playing"]  # å¼ºåŒ¹é…
base_confidence = 0.4

# å†³ç­–è®¡ç®—
type_boost = 0.2      # é€‰æ‹©é¢˜é€‚åˆè€ƒæŸ¥æ—¶æ€
keyword_boost = 0.15  # å¤šä¸ªå…³é”®è¯åŒ¹é…
history_boost = 0.1
difficulty_boost = 0.1
penalty = 0.0

decision_score = 0.4 + 0.2 + 0.15 + 0.1 + 0.1 = 0.85

final_weight = 0.85
auto_applied = True   # 0.85 >= 0.7ï¼Œè‡ªåŠ¨åº”ç”¨ï¼
```

## å‚æ•°è°ƒä¼˜æŒ‡å—

### ğŸ›ï¸ å…³é”®å‚æ•°è¯´æ˜

#### confidence_threshold (ç½®ä¿¡åº¦é˜ˆå€¼)
- **é»˜è®¤å€¼**: 0.3
- **ä½œç”¨**: åªæœ‰å†³ç­–åˆ†æ•°è¶…è¿‡æ­¤é˜ˆå€¼çš„æ ‡æ³¨æ‰ä¼šè¢«æ¨è
- **è°ƒä¼˜ç­–ç•¥**:
  - **æé«˜é˜ˆå€¼** (0.4-0.6): æ›´ä¿å®ˆï¼Œå‡†ç¡®ç‡é«˜ä½†è¦†ç›–ç‡ä½
  - **é™ä½é˜ˆå€¼** (0.1-0.2): æ›´æ¿€è¿›ï¼Œè¦†ç›–ç‡é«˜ä½†å¯èƒ½è¯¯æ ‡

```python
# ä¿å®ˆç­–ç•¥ - é«˜å‡†ç¡®ç‡
ai_agent_service.update_configuration({
    "confidence_threshold": 0.5
})

# æ¿€è¿›ç­–ç•¥ - é«˜è¦†ç›–ç‡
ai_agent_service.update_configuration({
    "confidence_threshold": 0.15
})
```

#### max_auto_annotations (æœ€å¤§è‡ªåŠ¨æ ‡æ³¨æ•°)
- **é»˜è®¤å€¼**: 5
- **ä½œç”¨**: é™åˆ¶æ¯é“é¢˜ç›®çš„æœ€å¤§æ ‡æ³¨æ•°é‡
- **è°ƒä¼˜ç­–ç•¥**:
  - **ç®€å•é¢˜ç›®**: è®¾ç½®ä¸º2-3ä¸ª
  - **å¤æ‚é¢˜ç›®**: è®¾ç½®ä¸º3-5ä¸ª

#### auto_apply_threshold (è‡ªåŠ¨åº”ç”¨é˜ˆå€¼)
- **é»˜è®¤å€¼**: 0.7
- **ä½œç”¨**: å†³ç­–åˆ†æ•°è¶…è¿‡æ­¤å€¼æ—¶è‡ªåŠ¨åº”ç”¨æ ‡æ³¨
- **è°ƒä¼˜ç­–ç•¥**:
  - **ä¸¥æ ¼æ¨¡å¼**: 0.8-0.9 (å‡ ä¹ä¸è‡ªåŠ¨åº”ç”¨)
  - **å¹³è¡¡æ¨¡å¼**: 0.6-0.7 (é€‚åº¦è‡ªåŠ¨åº”ç”¨)
  - **æ¿€è¿›æ¨¡å¼**: 0.4-0.5 (å¤§é‡è‡ªåŠ¨åº”ç”¨)

### ğŸ”§ æƒé‡å› å­è°ƒä¼˜

#### è°ƒæ•´å„å› å­æƒé‡
```python
# åœ¨_calculate_decision_scoreä¸­è°ƒæ•´æƒé‡
def _calculate_decision_score(self, question, suggestion, base_confidence):
    score = base_confidence
    
    # å¯ä»¥è°ƒæ•´è¿™äº›ç³»æ•°æ¥æ”¹å˜å„å› å­çš„å½±å“åŠ›
    score += self._get_question_type_boost(...) * 1.5      # æé«˜é¢˜ç›®ç±»å‹çš„é‡è¦æ€§
    score += self._get_keyword_match_boost(...) * 2.0      # æé«˜å…³é”®è¯åŒ¹é…çš„é‡è¦æ€§
    score += self._get_historical_accuracy_boost(...) * 0.5 # é™ä½å†å²æ•°æ®çš„å½±å“
    
    return max(0.0, min(1.0, score))
```

#### å…³é”®è¯æƒé‡è°ƒä¼˜
```python
# åœ¨_keyword_matching_scoreä¸­è°ƒæ•´æƒé‡
for pattern in patterns:
    if pattern.lower() in question_lower:
        matched_keywords.append(pattern)
        
        # å¯ä»¥æ ¹æ®å…³é”®è¯é‡è¦æ€§ç»™ä¸åŒæƒé‡
        if pattern in ["already", "yesterday", "every day"]:  # å¼ºæ ‡å¿—è¯
            score += 3.0
        elif len(pattern) > 5:  # é•¿å…³é”®è¯
            score += 2.0  
        else:                   # æ™®é€šå…³é”®è¯
            score += 1.0
```

### ğŸ“Š æ€§èƒ½ç›‘æ§å’Œä¼˜åŒ–

#### å‡†ç¡®ç‡ç›‘æ§
```python
def monitor_accuracy():
    # å®šæœŸè®¡ç®—å‡†ç¡®ç‡
    accuracy_data = analytics_service.get_ai_agent_accuracy_analysis()
    current_accuracy = accuracy_data['accuracy_analysis']['accuracy_rate']
    
    # å¦‚æœå‡†ç¡®ç‡ä¸‹é™ï¼Œè‡ªåŠ¨è°ƒæ•´å‚æ•°
    if current_accuracy < 60:
        # æé«˜ç½®ä¿¡åº¦é˜ˆå€¼
        ai_agent_service.update_configuration({
            "confidence_threshold": min(ai_agent_service.confidence_threshold + 0.1, 0.8)
        })
```

#### A/Bæµ‹è¯•æ¡†æ¶
```python
class ABTestFramework:
    def __init__(self):
        self.test_groups = {
            "conservative": {"confidence_threshold": 0.5, "type_boost_factor": 1.0},
            "aggressive": {"confidence_threshold": 0.2, "type_boost_factor": 1.5},
            "balanced": {"confidence_threshold": 0.3, "type_boost_factor": 1.2}
        }
    
    def run_ab_test(self, questions: List[Question]):
        results = {}
        for group_name, config in self.test_groups.items():
            # ä½¿ç”¨ä¸åŒé…ç½®æµ‹è¯•ç›¸åŒé¢˜ç›®
            results[group_name] = self.test_with_config(questions, config)
        return results
```

### ğŸ¯ å®é™…è°ƒä¼˜å»ºè®®

#### 1. æ ¹æ®æ•°æ®ç‰¹ç‚¹è°ƒä¼˜
```python
# å¦‚æœé¢˜ç›®ä»¥é€‰æ‹©é¢˜ä¸ºä¸»
type_mappings["é€‰æ‹©é¢˜"] = {
    "è¯­æ³•": 0.3,  # æé«˜è¯­æ³•æƒé‡
    "æ—¶æ€": 0.3,  # æé«˜æ—¶æ€æƒé‡
    "è¯æ±‡": 0.2
}

# å¦‚æœé¢˜ç›®ä»¥å¡«ç©ºé¢˜ä¸ºä¸»
type_mappings["å¡«ç©ºé¢˜"] = {
    "æ—¶æ€": 0.4,  # å¤§å¹…æé«˜æ—¶æ€æƒé‡
    "ä»‹è¯": 0.4,
    "è¯å½¢å˜åŒ–": 0.3
}
```

#### 2. åŸºäºå‡†ç¡®ç‡åé¦ˆè°ƒä¼˜
```python
def adaptive_tuning(accuracy_feedback):
    if accuracy_feedback < 70:
        # å‡†ç¡®ç‡ä½ï¼Œæé«˜é˜ˆå€¼
        self.confidence_threshold += 0.1
        
        # å¢åŠ å…³é”®è¯æƒé‡
        self.keyword_boost_factor = 1.5
    elif accuracy_feedback > 85:
        # å‡†ç¡®ç‡é«˜ï¼Œå¯ä»¥é™ä½é˜ˆå€¼æé«˜è¦†ç›–ç‡
        self.confidence_threshold -= 0.05
```

#### 3. åŠ¨æ€æƒé‡å­¦ä¹ 
```python
def update_weights_from_feedback(user_feedback):
    """åŸºäºç”¨æˆ·åé¦ˆåŠ¨æ€è°ƒæ•´æƒé‡"""
    for feedback in user_feedback:
        question_type = feedback['question_type']
        knowledge_point = feedback['knowledge_point']
        is_correct = feedback['is_correct']
        
        if is_correct:
            # æ­£ç¡®æ ‡æ³¨ï¼Œå¢åŠ å¯¹åº”çš„æƒé‡
            self._increase_type_weight(question_type, knowledge_point, 0.05)
        else:
            # é”™è¯¯æ ‡æ³¨ï¼Œå‡å°‘å¯¹åº”çš„æƒé‡
            self._decrease_type_weight(question_type, knowledge_point, 0.05)
```

### ğŸ“ˆ æ€§èƒ½ä¼˜åŒ–æŠ€å·§

#### 1. ç¼“å­˜æœºåˆ¶
```python
from functools import lru_cache

@lru_cache(maxsize=1000)
def _cached_keyword_matching(self, question_text: str, knowledge_point: str):
    """ç¼“å­˜å…³é”®è¯åŒ¹é…ç»“æœ"""
    return self._keyword_matching_score(question_text, knowledge_point)
```

#### 2. æ‰¹é‡ä¼˜åŒ–
```python
def batch_decision_scoring(self, questions: List[Question], suggestions_batch: List[List[Dict]]):
    """æ‰¹é‡è®¡ç®—å†³ç­–åˆ†æ•°ï¼Œæé«˜æ•ˆç‡"""
    scores = []
    
    # é¢„è®¡ç®—å…±åŒçš„æƒé‡å› å­
    type_boost_cache = {}
    
    for question, suggestions in zip(questions, suggestions_batch):
        question_scores = []
        for suggestion in suggestions:
            # ä½¿ç”¨ç¼“å­˜çš„æƒé‡å› å­
            score = self._fast_calculate_decision_score(question, suggestion, type_boost_cache)
            question_scores.append(score)
        scores.append(question_scores)
    
    return scores
```

#### 3. å¹¶è¡Œå¤„ç†
```python
import asyncio
from concurrent.futures import ThreadPoolExecutor

async def parallel_annotation(self, questions: List[Question]):
    """å¹¶è¡Œå¤„ç†å¤šä¸ªé¢˜ç›®çš„æ ‡æ³¨"""
    with ThreadPoolExecutor(max_workers=4) as executor:
        loop = asyncio.get_event_loop()
        
        tasks = [
            loop.run_in_executor(executor, self.auto_annotate_question, question)
            for question in questions
        ]
        
        results = await asyncio.gather(*tasks)
    return results
```

---

## ğŸš€ æ€»ç»“

AI Agentçš„æ ‡æ³¨å’Œæƒé‡è®¡ç®—æ˜¯ä¸€ä¸ª**å¤šå±‚æ¬¡ã€å¤šå› ç´ çš„æ™ºèƒ½å†³ç­–ç³»ç»Ÿ**ï¼š

### ğŸ§  **æ ¸å¿ƒæ€æƒ³**
1. **å¤šç»´åº¦è¯„ä¼°**: ä¸ä¾èµ–å•ä¸€æŒ‡æ ‡ï¼Œç»¼åˆå¤šä¸ªå› ç´ åˆ¤æ–­
2. **åŠ¨æ€æƒé‡**: æ ¹æ®ä¸åŒæƒ…å†µè°ƒæ•´å„å› å­çš„é‡è¦æ€§
3. **è‡ªé€‚åº”å­¦ä¹ **: å¯ä»¥æ ¹æ®ç”¨æˆ·åé¦ˆæŒç»­ä¼˜åŒ–
4. **ä¿å®ˆä¸æ¿€è¿›å¹³è¡¡**: é€šè¿‡é˜ˆå€¼æ§åˆ¶è‡ªåŠ¨åŒ–ç¨‹åº¦

### ğŸ¯ **æƒé‡è®¡ç®—æœ¬è´¨**
- **åŸºç¡€åˆ†æ•°**: åæ˜ é¢˜ç›®ä¸çŸ¥è¯†ç‚¹çš„åŸºæœ¬ç›¸å…³æ€§
- **åŠ æƒå› å­**: æ ¹æ®ä¸Šä¸‹æ–‡å’Œç»éªŒè°ƒæ•´ç›¸å…³æ€§
- **æœ€ç»ˆæƒé‡**: ç»¼åˆæ‰€æœ‰å› ç´ çš„å¯ä¿¡åº¦è¯„ä¼°

### ğŸ’¡ **ä¼˜åŒ–æ–¹å‘**
1. **æ‰©å……å…³é”®è¯åº“**: æé«˜åŸºç¡€åŒ¹é…å‡†ç¡®æ€§
2. **è°ƒæ•´å› å­æƒé‡**: æ ¹æ®å®é™…æ•ˆæœä¼˜åŒ–å„ç»´åº¦é‡è¦æ€§  
3. **å¼•å…¥æœºå™¨å­¦ä¹ **: ä½¿ç”¨æ›´å…ˆè¿›çš„NLPæ¨¡å‹
4. **ç”¨æˆ·åé¦ˆå¾ªç¯**: å»ºç«‹æŒç»­å­¦ä¹ æœºåˆ¶

è¿™ä¸ªç³»ç»Ÿçš„è®¾è®¡ç†å¿µæ˜¯**"å¯è§£é‡Šçš„AI"** - æ¯ä¸ªæ ‡æ³¨å†³ç­–éƒ½æœ‰æ˜ç¡®çš„ç†ç”±å’Œè®¡ç®—è¿‡ç¨‹ï¼Œä¾¿äºç†è§£ã€è°ƒè¯•å’Œä¼˜åŒ–ï¼
