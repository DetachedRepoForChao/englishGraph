#!/usr/bin/env python3
"""
å¼€æºè¯åº“å¢å¼ºæ¨¡å—
ä½¿ç”¨NLTK WordNetå’ŒspaCyæ¥ä¸°å¯Œå…³é”®è¯åº“
"""

import nltk
import spacy
from typing import Dict, List, Set, Tuple
import logging

logger = logging.getLogger(__name__)

class WordNetEnhancer:
    """ä½¿ç”¨å¼€æºè¯åº“å¢å¼ºå…³é”®è¯åŒ¹é…"""
    
    def __init__(self):
        self.nlp = None
        self.wordnet_loaded = False
        self._initialize_nltk()
        self._initialize_spacy()
    
    def _initialize_nltk(self):
        """åˆå§‹åŒ–NLTK"""
        try:
            # ä¸‹è½½å¿…è¦çš„NLTKæ•°æ®
            nltk.download('wordnet', quiet=True)
            nltk.download('brown', quiet=True)
            nltk.download('reuters', quiet=True)
            nltk.download('punkt', quiet=True)
            nltk.download('averaged_perceptron_tagger', quiet=True)
            self.wordnet_loaded = True
            logger.info("NLTK WordNet åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            logger.warning(f"NLTK åˆå§‹åŒ–å¤±è´¥: {e}")
    
    def _initialize_spacy(self):
        """åˆå§‹åŒ–spaCy"""
        try:
            self.nlp = spacy.load("en_core_web_sm")
            logger.info("spaCy æ¨¡å‹åŠ è½½æˆåŠŸ")
        except OSError:
            logger.warning("spaCy è‹±æ–‡æ¨¡å‹æœªå®‰è£…ï¼Œè¯·è¿è¡Œ: python -m spacy download en_core_web_sm")
        except Exception as e:
            logger.warning(f"spaCy åˆå§‹åŒ–å¤±è´¥: {e}")
    
    def get_synonyms(self, word: str) -> Set[str]:
        """è·å–åŒä¹‰è¯"""
        synonyms = set()
        if not self.wordnet_loaded:
            return synonyms
        
        try:
            from nltk.corpus import wordnet
            for syn in wordnet.synsets(word):
                for lemma in syn.lemmas():
                    synonyms.add(lemma.name().replace('_', ' '))
        except Exception as e:
            logger.warning(f"è·å–åŒä¹‰è¯å¤±è´¥ {word}: {e}")
        
        return synonyms
    
    def get_related_words(self, word: str) -> Set[str]:
        """è·å–ç›¸å…³è¯æ±‡"""
        related = set()
        if not self.wordnet_loaded:
            return related
        
        try:
            from nltk.corpus import wordnet
            for syn in wordnet.synsets(word):
                # ä¸Šä½è¯
                for hyper in syn.hypernyms():
                    for lemma in hyper.lemmas():
                        related.add(lemma.name().replace('_', ' '))
                # ä¸‹ä½è¯
                for hypo in syn.hyponyms():
                    for lemma in hypo.lemmas():
                        related.add(lemma.name().replace('_', ' '))
                # éƒ¨åˆ†è¯
                for part in syn.part_meronyms():
                    for lemma in part.lemmas():
                        related.add(lemma.name().replace('_', ' '))
        except Exception as e:
            logger.warning(f"è·å–ç›¸å…³è¯æ±‡å¤±è´¥ {word}: {e}")
        
        return related
    
    def get_grammatical_forms(self, word: str) -> Set[str]:
        """è·å–è¯­æ³•å½¢å¼å˜åŒ–"""
        forms = set()
        if not self.nlp:
            return forms
        
        try:
            doc = self.nlp(word)
            for token in doc:
                # è¯æ ¹å½¢å¼
                forms.add(token.lemma_)
                # è¯æ€§æ ‡æ³¨
                forms.add(f"{token.lemma_}_{token.pos_}")
        except Exception as e:
            logger.warning(f"è·å–è¯­æ³•å½¢å¼å¤±è´¥ {word}: {e}")
        
        return forms
    
    def enhance_knowledge_point_keywords(self, knowledge_point: str, base_keywords: List[str]) -> List[str]:
        """å¢å¼ºçŸ¥è¯†ç‚¹çš„å…³é”®è¯åº“"""
        enhanced_keywords = set(base_keywords)
        
        # ä¸ºæ¯ä¸ªåŸºç¡€å…³é”®è¯ç”Ÿæˆå¢å¼ºè¯æ±‡
        for keyword in base_keywords:
            # åŒä¹‰è¯
            synonyms = self.get_synonyms(keyword)
            enhanced_keywords.update(synonyms)
            
            # ç›¸å…³è¯æ±‡
            related = self.get_related_words(keyword)
            enhanced_keywords.update(related)
            
            # è¯­æ³•å½¢å¼
            forms = self.get_grammatical_forms(keyword)
            enhanced_keywords.update(forms)
        
        # è¿‡æ»¤å’Œæ¸…ç†
        filtered_keywords = self._filter_keywords(enhanced_keywords, knowledge_point)
        
        logger.info(f"çŸ¥è¯†ç‚¹ '{knowledge_point}' å…³é”®è¯ä» {len(base_keywords)} ä¸ªå¢å¼ºåˆ° {len(filtered_keywords)} ä¸ª")
        return list(filtered_keywords)
    
    def _filter_keywords(self, keywords: Set[str], knowledge_point: str) -> Set[str]:
        """è¿‡æ»¤å…³é”®è¯"""
        filtered = set()
        
        for keyword in keywords:
            # é•¿åº¦è¿‡æ»¤
            if len(keyword) < 2 or len(keyword) > 50:
                continue
            
            # ç‰¹æ®Šå­—ç¬¦è¿‡æ»¤
            if any(char in keyword for char in ['<', '>', '{', '}', '[', ']', '(', ')']):
                continue
            
            # æ•°å­—è¿‡æ»¤ï¼ˆä¿ç•™ä¸€äº›é‡è¦çš„ï¼‰
            if keyword.isdigit() and int(keyword) > 100:
                continue
            
            # ç›¸å…³æ€§è¿‡æ»¤ï¼ˆç®€å•å®ç°ï¼‰
            if self._is_relevant(keyword, knowledge_point):
                filtered.add(keyword.lower())
        
        return filtered
    
    def _is_relevant(self, keyword: str, knowledge_point: str) -> bool:
        """åˆ¤æ–­å…³é”®è¯æ˜¯å¦ç›¸å…³"""
        # ç®€å•çš„ç›¸å…³æ€§åˆ¤æ–­
        knowledge_lower = knowledge_point.lower()
        
        # åŒ…å«çŸ¥è¯†ç‚¹çš„æ ¸å¿ƒè¯æ±‡
        core_words = ['æ—¶æ€', 'è¯­æ€', 'ä»å¥', 'è¯­æ³•', 'è¯æ±‡', 'æ¯”è¾ƒ', 'è¢«åŠ¨', 'è¿›è¡Œ', 'å®Œæˆ', 'è¿‡å»', 'ç°åœ¨', 'å°†æ¥']
        if any(word in keyword.lower() for word in core_words):
            return True
        
        # åŒ…å«çŸ¥è¯†ç‚¹çš„è‹±æ–‡å…³é”®è¯
        english_core = ['tense', 'voice', 'clause', 'grammar', 'comparison', 'passive', 'progressive', 'perfect', 'past', 'present', 'future']
        if any(word in keyword.lower() for word in english_core):
            return True
        
        return True  # é»˜è®¤ä¿ç•™ï¼Œè®©åç»­å¤„ç†å†³å®š
    
    def generate_enhanced_keyword_patterns(self, base_patterns: Dict[str, List[str]]) -> Dict[str, List[str]]:
        """ç”Ÿæˆå¢å¼ºçš„å…³é”®è¯æ¨¡å¼åº“"""
        enhanced_patterns = {}
        
        for knowledge_point, keywords in base_patterns.items():
            enhanced_keywords = self.enhance_knowledge_point_keywords(knowledge_point, keywords)
            enhanced_patterns[knowledge_point] = enhanced_keywords
        
        return enhanced_patterns

# ä½¿ç”¨ç¤ºä¾‹
def demo_wordnet_enhancement():
    """æ¼”ç¤ºè¯åº“å¢å¼ºåŠŸèƒ½"""
    enhancer = WordNetEnhancer()
    
    # æµ‹è¯•åŒä¹‰è¯
    print("ğŸ” åŒä¹‰è¯æµ‹è¯•:")
    synonyms = enhancer.get_synonyms("play")
    print(f"play çš„åŒä¹‰è¯: {list(synonyms)[:10]}")
    
    # æµ‹è¯•ç›¸å…³è¯æ±‡
    print("\nğŸ”— ç›¸å…³è¯æ±‡æµ‹è¯•:")
    related = enhancer.get_related_words("tense")
    print(f"tense çš„ç›¸å…³è¯æ±‡: {list(related)[:10]}")
    
    # æµ‹è¯•è¯­æ³•å½¢å¼
    print("\nğŸ“ è¯­æ³•å½¢å¼æµ‹è¯•:")
    forms = enhancer.get_grammatical_forms("playing")
    print(f"playing çš„è¯­æ³•å½¢å¼: {list(forms)}")
    
    # æµ‹è¯•çŸ¥è¯†ç‚¹å¢å¼º
    print("\nğŸ¯ çŸ¥è¯†ç‚¹å¢å¼ºæµ‹è¯•:")
    base_keywords = ["play", "playing", "plays", "played"]
    enhanced = enhancer.enhance_knowledge_point_keywords("ç°åœ¨è¿›è¡Œæ—¶", base_keywords)
    print(f"ç°åœ¨è¿›è¡Œæ—¶å¢å¼ºå…³é”®è¯: {enhanced[:20]}")

if __name__ == "__main__":
    demo_wordnet_enhancement()
